{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e8ea6fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(os.path.dirname(os.path.abspath(\"__file__\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "79e482b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Git_Tutorials\\IntentClassification\\.venv\\Scripts\\python.exe\n",
      "['C:\\\\Users\\\\bsaar\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\python312.zip', 'C:\\\\Users\\\\bsaar\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\DLLs', 'C:\\\\Users\\\\bsaar\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\Lib', 'C:\\\\Users\\\\bsaar\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312', 'c:\\\\Git_Tutorials\\\\IntentClassification\\\\.venv', '', 'c:\\\\Git_Tutorials\\\\IntentClassification\\\\.venv\\\\Lib\\\\site-packages', 'c:\\\\Git_Tutorials\\\\IntentClassification\\\\.venv\\\\Lib\\\\site-packages\\\\win32', 'c:\\\\Git_Tutorials\\\\IntentClassification\\\\.venv\\\\Lib\\\\site-packages\\\\win32\\\\lib', 'c:\\\\Git_Tutorials\\\\IntentClassification\\\\.venv\\\\Lib\\\\site-packages\\\\Pythonwin']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f3724e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pymongo\n",
    "from pymongo.mongo_client import MongoClient\n",
    "from pymongo.server_api import ServerApi\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "import requests\n",
    "from zipfile import ZipFile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7bccd2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to convert dataframe records to json\n",
    "def csv_to_json_converter(csv_file_path):\n",
    "    try:\n",
    "        local_df = pd.read_csv(csv_file_path)\n",
    "        local_df.reset_index(drop=True, inplace=True)\n",
    "        records = local_df.to_dict(orient='records')\n",
    "        return records\n",
    "    except Exception as e:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e350a64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#code to insert the jsons into mongodb\n",
    "\n",
    "def insert_data_mongodb(json_data, database, collection):\n",
    "    try:\n",
    "        #connect to MongoDB\n",
    "        load_dotenv()\n",
    "        client = MongoClient(os.getenv('MONGO_DB_URL'), server_api=ServerApi('1'))\n",
    "        client.admin.command('ping')\n",
    "        print(\"Connected to MongoDB\")\n",
    "        db = client[database]\n",
    "        colln = db[collection]\n",
    "        #insert the json data\n",
    "        colln.insert_many(json_data)\n",
    "        return(f\"Inserted {len(json_data)} records into {collection} collection in {database} database.\")\n",
    "    except Exception as e:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3fe3c95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_collection_as_csv(csv_file_path, database, collection):\n",
    "    try:\n",
    "        load_dotenv()\n",
    "        client = MongoClient(os.getenv('MONGO_DB_URL'), server_api=ServerApi('1'))\n",
    "        client.admin.command('ping')\n",
    "        print(\"Connected to MongoDB\")\n",
    "        db = client[database]\n",
    "        colln=db[collection]\n",
    "        \n",
    "        df = pd.DataFrame(list(colln.find()))\n",
    "        print(\"export successful\")\n",
    "        if \"_id\" in df.columns:\n",
    "            df.drop(columns=['_id'], inplace =True, axis=1)\n",
    "        if \"Unnamed: 0\" in df.columns:\n",
    "            df.drop(columns=['Unnamed: 0'], inplace =True, axis=1)\n",
    "        print(\"csv_file saved succesfully\")\n",
    "        df.to_csv(csv_file_path, index=False, header = True)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1ebda304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perform partial redemption using service web service\n"
     ]
    }
   ],
   "source": [
    "from src.IntentClassification.utils.common import read_yaml\n",
    "from src.IntentClassification import constants as const\n",
    "import re\n",
    "\n",
    "sentence = \"perform partial redemption using service dptdrecordarredemption\"\n",
    "\n",
    "sentence = re.sub(r'\\bdp\\w+','web service',sentence)\n",
    "print(sentence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf104132",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_glove_embeddings(url, glove_folder, glove_zip_file):\n",
    "    if not os.path.exists(glove_folder):\n",
    "        os.makedirs(glove_folder, exist_ok=True)\n",
    "        response = requests.get(url, stream=True)\n",
    "        with open(glove_zip_file, 'wb') as file:\n",
    "            for chunk in response.iter_content(chunk_size=1024):\n",
    "                if chunk:\n",
    "                    file.write(chunk)\n",
    "        print(f\"Downloaded glove embeddings from {url}\")\n",
    "        with ZipFile(glove_zip_file, 'r') as zip:\n",
    "            zip.extractall(glove_folder)\n",
    "        print(f\"Unzipped glove embeddings to {glove_folder}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "621d98e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://nlp.stanford.edu/data/glove.6B.zip\"\n",
    "glove_folder = \"artifacts/glove\"\n",
    "glove_zip_file = \"artifacts/glove/glove.6B.zip\"\n",
    "\n",
    "download_glove_embeddings(url, glove_folder, glove_zip_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7c13407a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_embeddings(file_path=\"glove_data/glove.6B.100d.txt\"):\n",
    "    embeddings = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            word = parts[0]\n",
    "            vector = np.array(parts[1:], dtype=np.float32)\n",
    "            embeddings[word] = vector\n",
    "    print(f\"Loaded {len(embeddings)} word vectors.\")\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5842d2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "glove_path = \"artifacts/glove/glove.6B.100d.txt\"\n",
    "\n",
    "glove_embeddings = load_glove_embeddings(glove_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "89033955",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "df = pd.read_csv('artifacts/data_preprocessing/preprocessed.csv')\n",
    "corpus = df['preprocessed'].astype(str).tolist()\n",
    "\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
    "idf_scores = dict(zip(tfidf_vectorizer.get_feature_names_out(),tfidf_vectorizer.idf_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6543c5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_weighted_glove(sentence, glove_embeddings, idf_scores):\n",
    "    words = sentence.split()\n",
    "    vector = np.zeros(100)  # Assuming 100-dimensional GloVe embeddings\n",
    "    total_weight = 0\n",
    "    to_be_vectors = []\n",
    "\n",
    "    for word in words:\n",
    "        print(word)\n",
    "        if word in glove_embeddings and word in idf_scores:\n",
    "            weight = (idf_scores[word])\n",
    "            vector += glove_embeddings[word]*weight\n",
    "            total_weight += weight\n",
    "        else:\n",
    "            to_be_vectors.append(word)\n",
    "            print(len(to_be_vectors))\n",
    "            print(to_be_vectors)\n",
    "    return vector / total_weight if total_weight != 0 else vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a09f2372",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d57fe74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ['create a checking_account', 'place account_freeze restriction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e406cd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def tokenize_corpus(corpus):\n",
    "    return [word_tokenize(text) for text in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "efdeadc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_2_vec_path = \"artifacts/word2vec.model\"\n",
    "\n",
    "def save_word2vec_model(corpus, model_path):\n",
    "    model = Word2Vec(sentences=tokenize_corpus(corpus), vector_size=100, window=5, min_count=1, workers=4)\n",
    "    model.save(model_path)\n",
    "\n",
    "def load_word2vec_model(model_path):\n",
    "    return Word2Vec.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d640d470",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_word2vec_model(corpus, word_2_vec_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f5c46270",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_weighted_vectors(sentence, glove_embeddings, idf_scores, model_path):\n",
    "    words = word_tokenize(sentence)\n",
    "    vector = np.zeros(100)  # Assuming 100-dimensional GloVe embeddings\n",
    "    total_weight = 0\n",
    "\n",
    "    word_2_vec = load_word2vec_model(model_path)\n",
    "\n",
    "    for word in words:\n",
    "        \n",
    "        if word in glove_embeddings and word in idf_scores:\n",
    "            weight = (idf_scores[word])\n",
    "            vector += glove_embeddings[word]*weight\n",
    "            total_weight += weight\n",
    "        else:\n",
    "            print(word)\n",
    "            vector += word_2_vec.wv[word]\n",
    "    return vector / total_weight if total_weight != 0 else vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "4418f6f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "checking_account\n",
      "account_freeze\n"
     ]
    }
   ],
   "source": [
    "vec = {}\n",
    "for sentence in corpus:\n",
    "    vector = sentence_to_weighted_vectors(sentence, glove_embeddings, idf_scores, word_2_vec_path)\n",
    "    vec[sentence] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "6d86289f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: create a checking_account\n",
      "Vector: [-2.68656872e-01  2.67037610e-01 -3.51864134e-01  8.19922793e-02\n",
      "  9.10801804e-01 -4.83896640e-02 -4.56233171e-01 -2.22512343e-01\n",
      "  3.05263497e-01  3.32670408e-01 -4.67534921e-01 -2.08155619e-01\n",
      " -1.91024881e-01 -6.11630179e-01 -4.72010342e-02 -1.21737681e-01\n",
      "  4.24713741e-01  5.06791987e-01 -3.23761464e-01  1.66680504e-01\n",
      " -1.51868938e-02 -4.85195918e-01  4.69546150e-01 -5.55205749e-01\n",
      " -2.81407719e-01 -6.52552732e-01 -1.73183683e-01 -2.77245815e-01\n",
      " -1.56585879e-01 -4.37331469e-01 -1.81372937e-01  7.64171369e-01\n",
      " -5.23027677e-01 -5.85700844e-01  2.47193163e-01 -2.93478344e-01\n",
      "  4.98906516e-01 -3.41608906e-01 -2.72283059e-01 -8.71270059e-02\n",
      " -1.01984510e-01 -1.68437910e-02 -7.93050252e-01 -6.78542951e-01\n",
      " -2.52867106e-01  2.28658831e-01  7.67879690e-02  1.81167688e-01\n",
      " -3.95149896e-01 -6.43547988e-01 -5.48744317e-01 -1.07221875e-01\n",
      " -2.59122911e-01  7.69842995e-01  4.03534690e-01 -2.81473645e+00\n",
      "  5.42393541e-01 -1.16019630e-01  1.52413132e+00  1.86722784e-01\n",
      "  3.26892094e-02  8.96703503e-01 -2.26283675e-01 -7.32640228e-02\n",
      "  9.63722172e-01 -1.70249328e-03  3.37249128e-01  2.10631864e-01\n",
      "  3.57709227e-01 -9.32752085e-01  7.39703131e-01 -1.77382030e-01\n",
      " -3.86025650e-01 -4.30231124e-01  4.30298758e-01  4.95406104e-02\n",
      " -8.13803273e-02 -6.86886960e-02 -6.53389894e-01  3.99127136e-01\n",
      "  6.21757710e-01 -3.00521009e-02 -3.31575303e-01  3.46080882e-01\n",
      " -1.11684238e+00  6.80957510e-01  5.67559475e-01  1.79711368e-01\n",
      " -1.34482995e-02  1.10074178e-01  3.50933964e-01  7.82101544e-02\n",
      " -2.56892304e-01 -9.92752729e-01  1.67297516e-01 -4.73638657e-01\n",
      " -5.84170760e-02 -6.70294925e-01  5.92910614e-01  1.56044048e-01]\n",
      "\n",
      "Sentence: place account_freeze restriction\n",
      "Vector: [-0.07112641 -0.20399487  0.11272218 -0.05037881 -0.25268523  0.17093539\n",
      " -0.36557556  0.28257701 -0.07444646 -0.02425569  0.09939277  0.10658317\n",
      "  0.09276198 -0.55301972  0.18372248  0.21743574  0.16763312  0.22094406\n",
      " -0.52830602 -0.2300434   0.26498117  0.09059204  0.19951774  0.08460523\n",
      "  0.31496059 -0.17937972 -0.02512067 -0.61453189 -0.39894012 -0.27807326\n",
      " -0.25920884  0.18329029  0.22558578  0.07443213 -0.15854407  0.33210747\n",
      " -0.00748116  0.38266076 -0.6200205  -0.27789799 -0.34379316 -0.49679146\n",
      "  0.48784856 -0.27359724  0.18498401 -0.57158952  0.29257369  0.10463987\n",
      " -0.26667791 -0.4103888   0.29317618  0.13502224 -0.3103115   0.90031406\n",
      " -0.36293104 -1.73108375 -0.39798915 -0.24660955  1.36730752  0.36424048\n",
      " -0.57372528  0.22429787  0.10072473  0.17198155  0.96907523  0.13420124\n",
      " -0.10460306 -0.32749327 -0.10168924 -0.58792561 -0.36661792 -0.66244696\n",
      "  0.53663632  0.18457175  0.30114039  0.0658569  -0.25926656 -0.47570721\n",
      " -0.4001113  -0.12054749  0.24899132 -0.62838825 -0.3694874   0.42428624\n",
      " -0.70423774  0.21408264  0.2476175  -0.23288357  0.36311482 -0.23479961\n",
      " -0.67888195 -0.17988484  0.290535    0.11549129 -0.2372904  -0.08780225\n",
      "  0.23096618 -0.23800745  0.32049458  0.2211824 ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for item, value in vec.items():\n",
    "    print(f\"Sentence: {item}\\nVector: {value}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
